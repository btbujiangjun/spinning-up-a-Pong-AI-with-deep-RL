{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import necessary modules from keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "# creates a generic neural network architecture\n",
    "model = Sequential()\n",
    "\n",
    "# hidden layer takes a pre-processed frame as input, and has 200 units\n",
    "model.add(Dense(units=200,input_dim=80*80, activation='relu', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(units=1, activation='sigmoid', kernel_initializer='RandomNormal'))\n",
    "\n",
    "# compile the model using traditional Machine Learning losses and optimizers\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "# gym initialization\n",
    "env = gym.make(\"Pong-v0\")\n",
    "observation = env.reset()\n",
    "prev_input = None\n",
    "\n",
    "# Macros\n",
    "UP_ACTION = 2\n",
    "DOWN_ACTION = 3\n",
    "\n",
    "# Hyperparameters\n",
    "gamma = 0.99\n",
    "\n",
    "# initialization of variables used in the main loop\n",
    "x_train, y_train, rewards = [],[],[]\n",
    "reward_sum = 0\n",
    "episode_nb = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading previous weights\n"
     ]
    }
   ],
   "source": [
    "from easy_tf_log import tflog\n",
    "from datetime import datetime\n",
    "from keras import callbacks\n",
    "import os\n",
    "\n",
    "# initialize variables\n",
    "resume = True\n",
    "running_reward = None\n",
    "epochs_before_saving = 10\n",
    "log_dir = './log' + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "\n",
    "# load pre-trained model if exist\n",
    "if (resume and os.path.isfile('my_model_weights.h5')):\n",
    "    print(\"loading previous weights\")\n",
    "    model.load_weights('my_model_weights.h5')\n",
    "    \n",
    "# add a callback tensorboard object to visualize learning\n",
    "tbCallBack = callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0,  \n",
    "          write_graph=True, write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At the end of episode 0 the total reward was : -6.0\n",
      "Epoch 1/1\n",
      "6466/6466 [==============================] - 4s 582us/step - loss: -0.0072 - acc: 0.9869\n",
      "At the end of episode 1 the total reward was : 1.0\n",
      "Epoch 1/1\n",
      "6350/6350 [==============================] - 4s 562us/step - loss: -0.0111 - acc: 0.9841\n",
      "At the end of episode 2 the total reward was : -10.0\n",
      "Epoch 1/1\n",
      "6630/6630 [==============================] - 4s 589us/step - loss: -0.0081 - acc: 0.9903\n",
      "At the end of episode 3 the total reward was : 9.0\n",
      "Epoch 1/1\n",
      "6220/6220 [==============================] - 4s 598us/step - loss: -0.0069 - acc: 0.9876\n",
      "At the end of episode 4 the total reward was : -5.0\n",
      "Epoch 1/1\n",
      "6373/6373 [==============================] - 4s 600us/step - loss: -0.0026 - acc: 0.9892\n",
      "At the end of episode 5 the total reward was : -6.0\n",
      "Epoch 1/1\n",
      "5188/5188 [==============================] - 3s 589us/step - loss: -0.0086 - acc: 0.9902\n",
      "At the end of episode 6 the total reward was : -1.0\n",
      "Epoch 1/1\n",
      "6957/6957 [==============================] - 4s 570us/step - loss: -0.0038 - acc: 0.9876\n",
      "At the end of episode 7 the total reward was : -9.0\n",
      "Epoch 1/1\n",
      "6498/6498 [==============================] - 4s 587us/step - loss: -0.0027 - acc: 0.9860\n",
      "At the end of episode 8 the total reward was : -16.0\n",
      "Epoch 1/1\n",
      "4170/4170 [==============================] - 2s 572us/step - loss: 6.1102e-04 - acc: 0.9894\n",
      "At the end of episode 9 the total reward was : 4.0\n",
      "Epoch 1/1\n",
      "6679/6679 [==============================] - 4s 569us/step - loss: -0.0041 - acc: 0.9879\n",
      "At the end of episode 10 the total reward was : 5.0\n",
      "Epoch 1/1\n",
      "7066/7066 [==============================] - 4s 573us/step - loss: -0.0053 - acc: 0.9900\n",
      "At the end of episode 11 the total reward was : -7.0\n",
      "Epoch 1/1\n",
      "6342/6342 [==============================] - 4s 567us/step - loss: 4.6803e-04 - acc: 0.9883\n",
      "At the end of episode 12 the total reward was : -5.0\n",
      "Epoch 1/1\n",
      "6116/6116 [==============================] - 3s 572us/step - loss: -0.0076 - acc: 0.9858\n",
      "At the end of episode 13 the total reward was : -6.0\n",
      "Epoch 1/1\n",
      "5775/5775 [==============================] - 3s 575us/step - loss: -0.0031 - acc: 0.9882\n",
      "At the end of episode 14 the total reward was : 1.0\n",
      "Epoch 1/1\n",
      "6882/6882 [==============================] - 4s 568us/step - loss: -0.0033 - acc: 0.9875\n",
      "At the end of episode 15 the total reward was : -1.0\n",
      "Epoch 1/1\n",
      "6925/6925 [==============================] - 4s 563us/step - loss: -0.0090 - acc: 0.9877\n",
      "At the end of episode 16 the total reward was : -7.0\n",
      "Epoch 1/1\n",
      "6594/6594 [==============================] - 4s 591us/step - loss: -0.0056 - acc: 0.9857\n",
      "At the end of episode 17 the total reward was : -5.0\n",
      "Epoch 1/1\n",
      "7490/7490 [==============================] - 5s 653us/step - loss: -0.0060 - acc: 0.9874\n",
      "At the end of episode 18 the total reward was : 1.0\n",
      "Epoch 1/1\n",
      "6393/6393 [==============================] - 4s 569us/step - loss: -0.0091 - acc: 0.9894\n",
      "At the end of episode 19 the total reward was : -4.0\n",
      "Epoch 1/1\n",
      "6765/6765 [==============================] - 4s 540us/step - loss: -0.0053 - acc: 0.9873\n",
      "At the end of episode 20 the total reward was : 9.0\n",
      "Epoch 1/1\n",
      "5660/5660 [==============================] - 4s 667us/step - loss: -0.0034 - acc: 0.9892\n",
      "At the end of episode 21 the total reward was : -9.0\n",
      "Epoch 1/1\n",
      "6249/6249 [==============================] - 3s 554us/step - loss: -0.0064 - acc: 0.9856\n",
      "At the end of episode 22 the total reward was : -3.0\n",
      "Epoch 1/1\n",
      "6078/6078 [==============================] - 4s 601us/step - loss: -0.0049 - acc: 0.9878\n",
      "At the end of episode 23 the total reward was : 3.0\n",
      "Epoch 1/1\n",
      "6397/6397 [==============================] - 4s 565us/step - loss: -0.0038 - acc: 0.9870\n",
      "At the end of episode 24 the total reward was : 3.0\n",
      "Epoch 1/1\n",
      "6599/6599 [==============================] - 4s 541us/step - loss: -0.0053 - acc: 0.9859\n",
      "At the end of episode 25 the total reward was : -8.0\n",
      "Epoch 1/1\n",
      "5172/5172 [==============================] - 3s 567us/step - loss: -0.0035 - acc: 0.9896\n",
      "At the end of episode 26 the total reward was : -9.0\n",
      "Epoch 1/1\n",
      "5347/5347 [==============================] - 3s 541us/step - loss: -0.0069 - acc: 0.9865\n",
      "At the end of episode 27 the total reward was : -7.0\n",
      "Epoch 1/1\n",
      "5959/5959 [==============================] - 3s 542us/step - loss: -0.0026 - acc: 0.9923\n",
      "At the end of episode 28 the total reward was : -4.0\n",
      "Epoch 1/1\n",
      "6669/6669 [==============================] - 4s 547us/step - loss: -0.0042 - acc: 0.9862\n",
      "At the end of episode 29 the total reward was : 5.0\n",
      "Epoch 1/1\n",
      "4847/4847 [==============================] - 3s 538us/step - loss: -0.0028 - acc: 0.9899\n",
      "At the end of episode 30 the total reward was : 4.0\n",
      "Epoch 1/1\n",
      "6380/6380 [==============================] - 3s 540us/step - loss: -0.0105 - acc: 0.9871\n",
      "At the end of episode 31 the total reward was : -5.0\n",
      "Epoch 1/1\n",
      "6705/6705 [==============================] - 4s 539us/step - loss: -0.0048 - acc: 0.9857\n",
      "At the end of episode 32 the total reward was : 2.0\n",
      "Epoch 1/1\n",
      "6933/6933 [==============================] - 4s 582us/step - loss: -0.0109 - acc: 0.9886\n",
      "At the end of episode 33 the total reward was : 7.0\n",
      "Epoch 1/1\n",
      "7353/7353 [==============================] - 5s 622us/step - loss: -0.0060 - acc: 0.9879\n",
      "At the end of episode 34 the total reward was : -8.0\n",
      "Epoch 1/1\n",
      "4663/4663 [==============================] - 3s 570us/step - loss: -0.0080 - acc: 0.9865\n",
      "At the end of episode 35 the total reward was : -1.0\n",
      "Epoch 1/1\n",
      "6363/6363 [==============================] - 4s 581us/step - loss: -0.0028 - acc: 0.9890\n",
      "At the end of episode 36 the total reward was : -2.0\n",
      "Epoch 1/1\n",
      "6676/6676 [==============================] - 4s 580us/step - loss: -0.0033 - acc: 0.9891\n",
      "At the end of episode 37 the total reward was : -5.0\n",
      "Epoch 1/1\n",
      "6287/6287 [==============================] - 4s 573us/step - loss: -0.0036 - acc: 0.9884\n",
      "At the end of episode 38 the total reward was : -4.0\n",
      "Epoch 1/1\n",
      "7236/7236 [==============================] - 4s 581us/step - loss: -0.0050 - acc: 0.9873\n",
      "At the end of episode 39 the total reward was : -7.0\n",
      "Epoch 1/1\n",
      "6223/6223 [==============================] - 4s 588us/step - loss: -0.0012 - acc: 0.9886\n",
      "At the end of episode 40 the total reward was : 4.0\n",
      "Epoch 1/1\n",
      "6791/6791 [==============================] - 4s 541us/step - loss: -0.0031 - acc: 0.9866\n",
      "At the end of episode 41 the total reward was : -6.0\n",
      "Epoch 1/1\n",
      "6318/6318 [==============================] - 4s 569us/step - loss: -0.0094 - acc: 0.9907\n",
      "At the end of episode 42 the total reward was : 4.0\n",
      "Epoch 1/1\n",
      "7281/7281 [==============================] - 5s 618us/step - loss: -0.0038 - acc: 0.9856\n",
      "At the end of episode 43 the total reward was : -7.0\n",
      "Epoch 1/1\n",
      "7100/7100 [==============================] - 4s 585us/step - loss: -0.0037 - acc: 0.9869\n",
      "At the end of episode 44 the total reward was : -4.0\n",
      "Epoch 1/1\n",
      "6691/6691 [==============================] - 4s 542us/step - loss: -0.0070 - acc: 0.9883\n",
      "At the end of episode 45 the total reward was : -6.0\n",
      "Epoch 1/1\n",
      "7326/7326 [==============================] - 4s 543us/step - loss: -6.0807e-04 - acc: 0.9873\n",
      "At the end of episode 46 the total reward was : -7.0\n",
      "Epoch 1/1\n",
      "7285/7285 [==============================] - 4s 547us/step - loss: -0.0055 - acc: 0.9857\n"
     ]
    }
   ],
   "source": [
    "from karpathy import prepro, discount_rewards\n",
    "\n",
    "# main loop\n",
    "while (True):\n",
    "\n",
    "    # preprocess the observation, set input as difference between images\n",
    "    cur_input = prepro(observation)\n",
    "    x = cur_input - prev_input if prev_input is not None else np.zeros(80 * 80)\n",
    "    prev_input = cur_input\n",
    "    \n",
    "    # forward the policy network and sample action according to the proba distribution\n",
    "    proba = model.predict(np.expand_dims(x, axis=1).T)\n",
    "    action = UP_ACTION if np.random.uniform() < proba else DOWN_ACTION\n",
    "    y = 1 if action == 2 else 0 # 0 and 1 are our labels\n",
    "\n",
    "    # log the input and label to train later\n",
    "    x_train.append(x)\n",
    "    y_train.append(y)\n",
    "\n",
    "    # do one step in our environment\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    rewards.append(reward)\n",
    "    reward_sum += reward\n",
    "    \n",
    "    # end of an episode\n",
    "    if done:\n",
    "        print('At the end of episode', episode_nb, 'the total reward was :', reward_sum)\n",
    "        \n",
    "        # increment episode number\n",
    "        episode_nb += 1\n",
    "        \n",
    "        # training\n",
    "        model.fit(x=np.vstack(x_train), y=np.vstack(y_train), verbose=1, callbacks=[tbCallBack], sample_weight=discount_rewards(rewards, gamma))\n",
    "        \n",
    "        # Saving the weights used by our model\n",
    "        if episode_nb % epochs_before_saving == 0:    \n",
    "            model.save_weights('my_model_weights' + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '.h5')\n",
    "        \n",
    "        # Log the reward\n",
    "        running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "        tflog('running_reward', running_reward, custom_dir=log_dir)\n",
    "        \n",
    "        # Reinitialization\n",
    "        x_train, y_train, rewards = [],[],[]\n",
    "        observation = env.reset()\n",
    "        reward_sum = 0\n",
    "        prev_input = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
