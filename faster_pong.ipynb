{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# credit: Karpathy. https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5\n",
    "# normal imports\n",
    "import gym\n",
    "import time\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Activation\n",
    "from keras.models import Model, Sequential\n",
    "from keras.models import load_model\n",
    "from keras.utils import plot_model\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "import os.path\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from easy_tf_log import tflog\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model initialization\n",
    "def initialization(nb_hidden_layer_neurons, input_dimensionality, learning_rate, decay_rate):\n",
    "    model = Sequential()\n",
    "    # added regularizer l2 (improvement from )\n",
    "    model.add(Dense(units=nb_hidden_layer_neurons, input_dim=input_dimensionality, activation='relu', kernel_initializer='glorot_uniform'))#, kernel_regularizer=regularizers.l2(0.01)))\n",
    "    model.add(Dense(units=1, activation='sigmoid', kernel_initializer='RandomNormal')) # by default stddev=0.05 for RandomNorma\n",
    "    #rms_prop = keras.optimizers.RMSprop(lr=learning_rate, decay=decay_rate)\n",
    "    adam = keras.optimizers.Adam()\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing used by Karpathy (cf. https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5)\n",
    "def prepro(I):\n",
    "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "  I = I[35:195] # crop\n",
    "  I = I[::2,::2,0] # downsample by factor of 2\n",
    "  I[I == 144] = 0 # erase background (background type 1)\n",
    "  I[I == 109] = 0 # erase background (background type 2)\n",
    "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "  return I.astype(np.float).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reward discount used by Karpathy (cf. https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5)\n",
    "def discount_rewards(r):\n",
    "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "  discounted_r = np.zeros_like(r)\n",
    "  running_add = 0\n",
    "  # we go from last reward to first one so we don't have to do exponentiations\n",
    "  for t in reversed(range(0, r.size)):\n",
    "    if r[t] != 0: running_add = 0 # if the game ended (in Pong), reset the reward sum\n",
    "    running_add = running_add * gamma + r[t] # the point here is to use Horner's method to compute those rewards efficiently\n",
    "    discounted_r[t] = running_add\n",
    "  return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our custom loss\n",
    "def custom_loss(discounted_rewards):\n",
    "    def custom_loss_aux(y_true, y_pred):\n",
    "        return -custom_loss_sum(y_true, y_pred, discounted_rewards)\n",
    "    return custom_loss_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters, global variables\n",
    "nb_hidden_layer_neurons = 200  # number of hidden layer neurons\n",
    "batch_size = 1 # every how many episodes to do a param update?\n",
    "gamma = 0.99 # discount factor for reward\n",
    "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
    "input_dimensionality = 80 * 80 # input dimensionality: 80x80 grid\n",
    "learning_rate = 1e-4\n",
    "resume = True # resume from previous checkpoint?\n",
    "render = False # render the game\n",
    "nb_episodes = 10000\n",
    "\n",
    "UP_ACTION = 2\n",
    "DOWN_ACTION = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading previous weights\n",
      "episode  1 / 10000\n",
      "Epoch 1/1\n",
      "6836/6836 [==============================] - 4s 630us/step - loss: -0.0062 - acc: 0.9901\n",
      "running reward:  8.0  reward this episode:  8.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  2 / 10000\n",
      "Epoch 1/1\n",
      "9406/9406 [==============================] - 6s 593us/step - loss: -0.0070 - acc: 0.9876\n",
      "running reward:  7.9  reward this episode:  -2.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  3 / 10000\n",
      "Epoch 1/1\n",
      "7017/7017 [==============================] - 5s 661us/step - loss: -0.0051 - acc: 0.9880\n",
      "running reward:  7.791  reward this episode:  -3.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  4 / 10000\n",
      "Epoch 1/1\n",
      "6372/6372 [==============================] - 5s 745us/step - loss: -0.0064 - acc: 0.9873\n",
      "running reward:  7.66309  reward this episode:  -5.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  5 / 10000\n",
      "Epoch 1/1\n",
      "5301/5301 [==============================] - 3s 562us/step - loss: -0.0015 - acc: 0.9885\n",
      "running reward:  7.6664591  reward this episode:  8.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  6 / 10000\n",
      "Epoch 1/1\n",
      "7398/7398 [==============================] - 4s 556us/step - loss: -0.0023 - acc: 0.9886\n",
      "running reward:  7.569794509  reward this episode:  -2.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  7 / 10000\n",
      "Epoch 1/1\n",
      "7250/7250 [==============================] - 4s 554us/step - loss: -0.0107 - acc: 0.9859\n",
      "running reward:  7.434096563910001  reward this episode:  -6.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  8 / 10000\n",
      "Epoch 1/1\n",
      "6487/6487 [==============================] - 4s 558us/step - loss: -2.4286e-04 - acc: 0.9827\n",
      "running reward:  7.249755598270901  reward this episode:  -11.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  9 / 10000\n",
      "Epoch 1/1\n",
      "6760/6760 [==============================] - 4s 545us/step - loss: -0.0062 - acc: 0.9854\n",
      "running reward:  7.1472580422881915  reward this episode:  -3.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  10 / 10000\n",
      "Epoch 1/1\n",
      "7779/7779 [==============================] - 4s 558us/step - loss: -0.0073 - acc: 0.9837\n",
      "running reward:  7.045785461865309  reward this episode:  -3.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  11 / 10000\n",
      "Epoch 1/1\n",
      "7525/7525 [==============================] - 4s 542us/step - loss: -0.0031 - acc: 0.9887\n",
      "running reward:  6.945327607246655  reward this episode:  -3.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  12 / 10000\n",
      "Epoch 1/1\n",
      "7180/7180 [==============================] - 4s 555us/step - loss: -0.0065 - acc: 0.9869\n",
      "running reward:  6.845874331174189  reward this episode:  -3.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  13 / 10000\n",
      "Epoch 1/1\n",
      "7076/7076 [==============================] - 5s 670us/step - loss: -0.0046 - acc: 0.9880\n",
      "running reward:  6.747415587862447  reward this episode:  -3.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  14 / 10000\n",
      "Epoch 1/1\n",
      "7026/7026 [==============================] - 5s 742us/step - loss: -0.0082 - acc: 0.9868\n",
      "running reward:  6.699941431983822  reward this episode:  2.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  15 / 10000\n",
      "Epoch 1/1\n",
      "6233/6233 [==============================] - 4s 584us/step - loss: -0.0012 - acc: 0.9893\n",
      "running reward:  6.512942017663984  reward this episode:  -12.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  16 / 10000\n",
      "Epoch 1/1\n",
      "7170/7170 [==============================] - 4s 556us/step - loss: -0.0033 - acc: 0.9874\n",
      "running reward:  6.457812597487344  reward this episode:  1.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  17 / 10000\n",
      "Epoch 1/1\n",
      "6538/6538 [==============================] - 4s 555us/step - loss: -0.0036 - acc: 0.9884\n",
      "running reward:  6.36323447151247  reward this episode:  -3.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  18 / 10000\n",
      "Epoch 1/1\n",
      "6140/6140 [==============================] - 3s 547us/step - loss: -0.0020 - acc: 0.9891\n",
      "running reward:  6.239602126797346  reward this episode:  -6.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  19 / 10000\n",
      "Epoch 1/1\n",
      "5528/5528 [==============================] - 3s 607us/step - loss: -0.0081 - acc: 0.9857\n",
      "running reward:  6.087206105529372  reward this episode:  -9.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  20 / 10000\n",
      "Epoch 1/1\n",
      "5575/5575 [==============================] - 4s 662us/step - loss: -0.0098 - acc: 0.9871\n",
      "running reward:  5.896334044474078  reward this episode:  -13.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  21 / 10000\n",
      "Epoch 1/1\n",
      "7511/7511 [==============================] - 6s 750us/step - loss: -0.0078 - acc: 0.9858\n",
      "running reward:  5.807370704029337  reward this episode:  -3.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  22 / 10000\n",
      "Epoch 1/1\n",
      "8036/8036 [==============================] - 5s 624us/step - loss: -0.0042 - acc: 0.9898\n",
      "running reward:  5.739296996989044  reward this episode:  -1.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  23 / 10000\n",
      "Epoch 1/1\n",
      "8164/8164 [==============================] - 5s 612us/step - loss: -0.0086 - acc: 0.9864\n",
      "running reward:  5.651904027019153  reward this episode:  -3.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  24 / 10000\n",
      "Epoch 1/1\n",
      "7303/7303 [==============================] - 5s 674us/step - loss: -0.0080 - acc: 0.9853\n",
      "running reward:  5.555384986748962  reward this episode:  -4.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  25 / 10000\n",
      "Epoch 1/1\n",
      "5643/5643 [==============================] - 4s 667us/step - loss: -0.0064 - acc: 0.9862\n",
      "running reward:  5.3498311368814715  reward this episode:  -15.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  26 / 10000\n",
      "Epoch 1/1\n",
      "4947/4947 [==============================] - 3s 630us/step - loss: -0.0070 - acc: 0.9897\n",
      "running reward:  5.156332825512657  reward this episode:  -14.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  27 / 10000\n",
      "Epoch 1/1\n",
      "6294/6294 [==============================] - 4s 660us/step - loss: -0.0024 - acc: 0.9914\n",
      "running reward:  5.134769497257531  reward this episode:  3.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  28 / 10000\n",
      "Epoch 1/1\n",
      "5230/5230 [==============================] - 4s 692us/step - loss: -0.0083 - acc: 0.9866\n",
      "running reward:  4.9934218022849555  reward this episode:  -9.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  29 / 10000\n",
      "Epoch 1/1\n",
      "7591/7591 [==============================] - 5s 610us/step - loss: -0.0033 - acc: 0.9895\n",
      "running reward:  4.893487584262106  reward this episode:  -5.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  30 / 10000\n",
      "Epoch 1/1\n",
      "6433/6433 [==============================] - 4s 591us/step - loss: -0.0049 - acc: 0.9859\n",
      "running reward:  4.764552708419485  reward this episode:  -8.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  31 / 10000\n",
      "Epoch 1/1\n",
      "6700/6700 [==============================] - 4s 624us/step - loss: -0.0034 - acc: 0.9864\n",
      "running reward:  4.66690718133529  reward this episode:  -5.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  32 / 10000\n",
      "Epoch 1/1\n",
      "5772/5772 [==============================] - 3s 545us/step - loss: -0.0045 - acc: 0.9879\n",
      "running reward:  4.540238109521937  reward this episode:  -8.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  33 / 10000\n",
      "Epoch 1/1\n",
      "5416/5416 [==============================] - 3s 554us/step - loss: -0.0032 - acc: 0.9863\n",
      "running reward:  4.374835728426717  reward this episode:  -12.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  34 / 10000\n",
      "Epoch 1/1\n",
      "7263/7263 [==============================] - 4s 551us/step - loss: -0.0087 - acc: 0.9871\n",
      "running reward:  4.28108737114245  reward this episode:  -5.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  35 / 10000\n",
      "Epoch 1/1\n",
      "6847/6847 [==============================] - 4s 540us/step - loss: -0.0021 - acc: 0.9852\n",
      "running reward:  4.188276497431026  reward this episode:  -5.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  36 / 10000\n",
      "Epoch 1/1\n",
      "5829/5829 [==============================] - 3s 543us/step - loss: -0.0083 - acc: 0.9888\n",
      "running reward:  4.196393732456716  reward this episode:  5.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  37 / 10000\n",
      "Epoch 1/1\n",
      "6725/6725 [==============================] - 4s 558us/step - loss: -0.0062 - acc: 0.9887\n",
      "running reward:  4.204429795132149  reward this episode:  5.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  38 / 10000\n",
      "Epoch 1/1\n",
      "5017/5017 [==============================] - 3s 548us/step - loss: -0.0064 - acc: 0.9856\n",
      "running reward:  4.032385497180828  reward this episode:  -13.0\n",
      "logging into: ./log20181120-142143/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  39 / 10000\n",
      "Epoch 1/1\n",
      "5985/5985 [==============================] - 3s 543us/step - loss: -0.0074 - acc: 0.9866\n",
      "running reward:  3.9420616422090196  reward this episode:  -5.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  40 / 10000\n",
      "Epoch 1/1\n",
      "7109/7109 [==============================] - 4s 541us/step - loss: -0.0077 - acc: 0.9887\n",
      "running reward:  3.9226410257869295  reward this episode:  2.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  41 / 10000\n",
      "Epoch 1/1\n",
      "6766/6766 [==============================] - 4s 559us/step - loss: -0.0046 - acc: 0.9895\n",
      "running reward:  3.93341461552906  reward this episode:  5.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  42 / 10000\n",
      "Epoch 1/1\n",
      "6945/6945 [==============================] - 4s 557us/step - loss: -0.0078 - acc: 0.9875\n",
      "running reward:  3.8440804693737696  reward this episode:  -5.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  43 / 10000\n",
      "Epoch 1/1\n",
      "6778/6778 [==============================] - 4s 556us/step - loss: -0.0055 - acc: 0.9897\n",
      "running reward:  3.775639664680032  reward this episode:  -3.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  44 / 10000\n",
      "Epoch 1/1\n",
      "5129/5129 [==============================] - 3s 556us/step - loss: -0.0044 - acc: 0.9869\n",
      "running reward:  3.647883268033232  reward this episode:  -9.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  45 / 10000\n",
      "Epoch 1/1\n",
      "7808/7808 [==============================] - 4s 544us/step - loss: -0.0057 - acc: 0.9891\n",
      "running reward:  3.5514044353528993  reward this episode:  -6.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  46 / 10000\n",
      "Epoch 1/1\n",
      "6301/6301 [==============================] - 4s 559us/step - loss: -0.0078 - acc: 0.9887\n",
      "running reward:  3.4658903909993706  reward this episode:  -5.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  47 / 10000\n",
      "Epoch 1/1\n",
      "7184/7184 [==============================] - 4s 543us/step - loss: -0.0040 - acc: 0.9859\n",
      "running reward:  3.3512314870893767  reward this episode:  -8.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  48 / 10000\n",
      "Epoch 1/1\n",
      "6872/6872 [==============================] - 4s 547us/step - loss: -0.0016 - acc: 0.9897\n",
      "running reward:  3.357719172218483  reward this episode:  4.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  49 / 10000\n",
      "Epoch 1/1\n",
      "6388/6388 [==============================] - 3s 543us/step - loss: -0.0027 - acc: 0.9842\n",
      "running reward:  3.234141980496298  reward this episode:  -9.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  50 / 10000\n",
      "Epoch 1/1\n",
      "6610/6610 [==============================] - 4s 540us/step - loss: -0.0068 - acc: 0.9856\n",
      "running reward:  3.1518005606913353  reward this episode:  -5.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  51 / 10000\n",
      "Epoch 1/1\n",
      "6294/6294 [==============================] - 3s 548us/step - loss: -0.0080 - acc: 0.9868\n",
      "running reward:  3.050282555084422  reward this episode:  -7.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  52 / 10000\n",
      "Epoch 1/1\n",
      "5539/5539 [==============================] - 3s 542us/step - loss: -0.0092 - acc: 0.9910\n",
      "running reward:  3.099779729533578  reward this episode:  8.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  53 / 10000\n",
      "Epoch 1/1\n",
      "4889/4889 [==============================] - 3s 545us/step - loss: -0.0032 - acc: 0.9888\n",
      "running reward:  2.9587819322382423  reward this episode:  -11.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  54 / 10000\n",
      "Epoch 1/1\n",
      "7033/7033 [==============================] - 4s 543us/step - loss: -0.0075 - acc: 0.9871\n",
      "running reward:  2.8691941129158596  reward this episode:  -6.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  55 / 10000\n",
      "Epoch 1/1\n",
      "6657/6657 [==============================] - 4s 556us/step - loss: -0.0047 - acc: 0.9865\n",
      "running reward:  2.780502171786701  reward this episode:  -6.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  56 / 10000\n",
      "Epoch 1/1\n",
      "5362/5362 [==============================] - 3s 543us/step - loss: -0.0023 - acc: 0.9890\n",
      "running reward:  2.6626971500688343  reward this episode:  -9.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  57 / 10000\n",
      "Epoch 1/1\n",
      "7693/7693 [==============================] - 4s 541us/step - loss: -0.0108 - acc: 0.9893\n",
      "running reward:  2.616070178568146  reward this episode:  -2.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  58 / 10000\n",
      "Epoch 1/1\n",
      "7873/7873 [==============================] - 4s 547us/step - loss: -0.0046 - acc: 0.9877\n",
      "running reward:  2.5999094767824644  reward this episode:  1.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  59 / 10000\n",
      "Epoch 1/1\n",
      "5880/5880 [==============================] - 3s 559us/step - loss: -0.0047 - acc: 0.9845\n",
      "running reward:  2.4839103820146398  reward this episode:  -9.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  60 / 10000\n",
      "Epoch 1/1\n",
      "7625/7625 [==============================] - 4s 541us/step - loss: -0.0100 - acc: 0.9879\n",
      "running reward:  2.4990712781944935  reward this episode:  4.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  61 / 10000\n",
      "Epoch 1/1\n",
      "6581/6581 [==============================] - 4s 544us/step - loss: -0.0045 - acc: 0.9877\n",
      "running reward:  2.5240805654125484  reward this episode:  5.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  62 / 10000\n",
      "Epoch 1/1\n",
      "7759/7759 [==============================] - 4s 552us/step - loss: -0.0059 - acc: 0.9888\n",
      "running reward:  2.4688397597584233  reward this episode:  -3.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  63 / 10000\n",
      "Epoch 1/1\n",
      "6051/6051 [==============================] - 3s 547us/step - loss: -0.0070 - acc: 0.9864\n",
      "running reward:  2.3441513621608387  reward this episode:  -10.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  64 / 10000\n",
      "Epoch 1/1\n",
      "7033/7033 [==============================] - 4s 555us/step - loss: -0.0122 - acc: 0.9859\n",
      "running reward:  2.3107098485392306  reward this episode:  -1.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  65 / 10000\n",
      "Epoch 1/1\n",
      "6438/6438 [==============================] - 4s 552us/step - loss: -0.0106 - acc: 0.9842\n",
      "running reward:  2.2176027500538384  reward this episode:  -7.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  66 / 10000\n",
      "Epoch 1/1\n",
      "5143/5143 [==============================] - 3s 550us/step - loss: -0.0087 - acc: 0.9856\n",
      "running reward:  2.1054267225533003  reward this episode:  -9.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  67 / 10000\n",
      "Epoch 1/1\n",
      "6744/6744 [==============================] - 4s 549us/step - loss: -0.0048 - acc: 0.9880\n",
      "running reward:  2.064372455327767  reward this episode:  -2.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  68 / 10000\n",
      "Epoch 1/1\n",
      "7578/7578 [==============================] - 4s 543us/step - loss: -0.0048 - acc: 0.9859\n",
      "running reward:  1.9537287307744895  reward this episode:  -9.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  69 / 10000\n",
      "Epoch 1/1\n",
      "6594/6594 [==============================] - 4s 546us/step - loss: -0.0067 - acc: 0.9889\n",
      "running reward:  1.9441914434667447  reward this episode:  1.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  70 / 10000\n",
      "Epoch 1/1\n",
      "5720/5720 [==============================] - 3s 567us/step - loss: -0.0068 - acc: 0.9879\n",
      "running reward:  1.8047495290320774  reward this episode:  -12.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  71 / 10000\n",
      "Epoch 1/1\n",
      "4456/4456 [==============================] - 3s 580us/step - loss: -0.0048 - acc: 0.9838\n",
      "running reward:  1.6567020337417566  reward this episode:  -13.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  72 / 10000\n",
      "Epoch 1/1\n",
      "8685/8685 [==============================] - 5s 548us/step - loss: -0.0040 - acc: 0.9874\n",
      "running reward:  1.610135013404339  reward this episode:  -3.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  73 / 10000\n",
      "Epoch 1/1\n",
      "5987/5987 [==============================] - 3s 563us/step - loss: -0.0053 - acc: 0.9866\n",
      "running reward:  1.6940336632702957  reward this episode:  10.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  74 / 10000\n",
      "Epoch 1/1\n",
      "6463/6463 [==============================] - 4s 548us/step - loss: -0.0098 - acc: 0.9878\n",
      "running reward:  1.5970933266375926  reward this episode:  -8.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  75 / 10000\n",
      "Epoch 1/1\n",
      "7522/7522 [==============================] - 4s 562us/step - loss: -0.0043 - acc: 0.9864\n",
      "running reward:  1.5311223933712166  reward this episode:  -5.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  76 / 10000\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6177/6177 [==============================] - 3s 553us/step - loss: -0.0065 - acc: 0.9870\n",
      "running reward:  1.4958111694375045  reward this episode:  -2.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  77 / 10000\n",
      "Epoch 1/1\n",
      "6192/6192 [==============================] - 3s 558us/step - loss: -0.0016 - acc: 0.9855\n",
      "running reward:  1.3608530577431295  reward this episode:  -12.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  78 / 10000\n",
      "Epoch 1/1\n",
      "6008/6008 [==============================] - 3s 555us/step - loss: -0.0072 - acc: 0.9878\n",
      "running reward:  1.237244527165698  reward this episode:  -11.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  79 / 10000\n",
      "Epoch 1/1\n",
      "8014/8014 [==============================] - 4s 554us/step - loss: -0.0046 - acc: 0.9890\n",
      "running reward:  1.194872081894041  reward this episode:  -3.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  80 / 10000\n",
      "Epoch 1/1\n",
      "7196/7196 [==============================] - 4s 563us/step - loss: -0.0060 - acc: 0.9879\n",
      "running reward:  1.1929233610751007  reward this episode:  1.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  81 / 10000\n",
      "Epoch 1/1\n",
      "6704/6704 [==============================] - 4s 548us/step - loss: -0.0094 - acc: 0.9882\n",
      "running reward:  1.1709941274643496  reward this episode:  -1.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  82 / 10000\n",
      "Epoch 1/1\n",
      "6443/6443 [==============================] - 4s 544us/step - loss: -0.0017 - acc: 0.9849\n",
      "running reward:  1.029284186189706  reward this episode:  -13.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  83 / 10000\n",
      "Epoch 1/1\n",
      "6663/6663 [==============================] - 4s 548us/step - loss: -0.0040 - acc: 0.9857\n",
      "running reward:  0.9089913443278089  reward this episode:  -11.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  84 / 10000\n",
      "Epoch 1/1\n",
      "5559/5559 [==============================] - 3s 547us/step - loss: -0.0083 - acc: 0.9905\n",
      "running reward:  0.8099014308845308  reward this episode:  -9.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  85 / 10000\n",
      "Epoch 1/1\n",
      "6505/6505 [==============================] - 4s 544us/step - loss: -0.0058 - acc: 0.9892\n",
      "running reward:  0.7918024165756855  reward this episode:  -1.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  86 / 10000\n",
      "Epoch 1/1\n",
      "7044/7044 [==============================] - 4s 550us/step - loss: -0.0121 - acc: 0.9865\n",
      "running reward:  0.7538843924099287  reward this episode:  -3.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  87 / 10000\n",
      "Epoch 1/1\n",
      "6223/6223 [==============================] - 3s 552us/step - loss: -0.0081 - acc: 0.9916\n",
      "running reward:  0.8063455484858293  reward this episode:  6.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  88 / 10000\n",
      "Epoch 1/1\n",
      "6306/6306 [==============================] - 4s 557us/step - loss: -0.0066 - acc: 0.9851\n",
      "running reward:  0.738282093000971  reward this episode:  -6.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  89 / 10000\n",
      "Epoch 1/1\n",
      "8265/8265 [==============================] - 5s 549us/step - loss: -0.0075 - acc: 0.9869\n",
      "running reward:  0.7408992720709613  reward this episode:  1.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  90 / 10000\n",
      "Epoch 1/1\n",
      "5202/5202 [==============================] - 3s 543us/step - loss: -0.0072 - acc: 0.9848\n",
      "running reward:  0.6234902793502517  reward this episode:  -11.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  91 / 10000\n",
      "Epoch 1/1\n",
      "4579/4579 [==============================] - 2s 540us/step - loss: -0.0040 - acc: 0.9862\n",
      "running reward:  0.4572553765567492  reward this episode:  -16.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  92 / 10000\n",
      "Epoch 1/1\n",
      "7440/7440 [==============================] - 4s 549us/step - loss: -0.0054 - acc: 0.9860\n",
      "running reward:  0.47268282279118173  reward this episode:  2.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  93 / 10000\n",
      "Epoch 1/1\n",
      "6717/6717 [==============================] - 4s 543us/step - loss: -0.0070 - acc: 0.9856\n",
      "running reward:  0.4079559945632699  reward this episode:  -6.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  94 / 10000\n",
      "Epoch 1/1\n",
      "6796/6796 [==============================] - 4s 555us/step - loss: -0.0100 - acc: 0.9866\n",
      "running reward:  0.3438764346176372  reward this episode:  -6.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  95 / 10000\n",
      "Epoch 1/1\n",
      "7347/7347 [==============================] - 4s 542us/step - loss: -0.0064 - acc: 0.9884\n",
      "running reward:  0.2904376702714609  reward this episode:  -5.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  96 / 10000\n",
      "Epoch 1/1\n",
      "6025/6025 [==============================] - 3s 546us/step - loss: -0.0109 - acc: 0.9841\n",
      "running reward:  0.1775332935687463  reward this episode:  -11.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  97 / 10000\n",
      "Epoch 1/1\n",
      "6434/6434 [==============================] - 4s 550us/step - loss: -0.0035 - acc: 0.9876\n",
      "running reward:  0.12575796063305883  reward this episode:  -5.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  98 / 10000\n",
      "Epoch 1/1\n",
      "7469/7469 [==============================] - 4s 547us/step - loss: -0.0027 - acc: 0.9889\n",
      "running reward:  0.10450038102672823  reward this episode:  -2.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  99 / 10000\n",
      "Epoch 1/1\n",
      "7219/7219 [==============================] - 4s 558us/step - loss: -0.0085 - acc: 0.9875\n",
      "running reward:  0.11345537721646094  reward this episode:  1.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  100 / 10000\n",
      "Epoch 1/1\n",
      "4678/4678 [==============================] - 3s 547us/step - loss: -0.0073 - acc: 0.9857\n",
      "running reward:  -0.0076791765557036595  reward this episode:  -12.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  101 / 10000\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 3s 544us/step - loss: -0.0064 - acc: 0.9881\n",
      "running reward:  -0.09760238479014662  reward this episode:  -9.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  102 / 10000\n",
      "Epoch 1/1\n",
      "6808/6808 [==============================] - 4s 548us/step - loss: -0.0055 - acc: 0.9875\n",
      "running reward:  -0.20662636094224515  reward this episode:  -11.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  103 / 10000\n",
      "Epoch 1/1\n",
      "6776/6776 [==============================] - 4s 546us/step - loss: -0.0062 - acc: 0.9889\n",
      "running reward:  -0.2545600973328227  reward this episode:  -5.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  104 / 10000\n",
      "Epoch 1/1\n",
      "6036/6036 [==============================] - 3s 542us/step - loss: -0.0058 - acc: 0.9902\n",
      "running reward:  -0.3220144963594945  reward this episode:  -7.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  105 / 10000\n",
      "Epoch 1/1\n",
      "8680/8680 [==============================] - 5s 548us/step - loss: -0.0082 - acc: 0.9869\n",
      "running reward:  -0.34879435139589954  reward this episode:  -3.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  106 / 10000\n",
      "Epoch 1/1\n",
      "6161/6161 [==============================] - 3s 543us/step - loss: -0.0077 - acc: 0.9906\n",
      "running reward:  -0.38530640788194054  reward this episode:  -4.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  107 / 10000\n",
      "Epoch 1/1\n",
      "7658/7658 [==============================] - 4s 548us/step - loss: -0.0094 - acc: 0.9881\n",
      "running reward:  -0.39145334380312113  reward this episode:  -1.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  108 / 10000\n",
      "Epoch 1/1\n",
      "5826/5826 [==============================] - 3s 549us/step - loss: -0.0070 - acc: 0.9854\n",
      "running reward:  -0.4575388103650899  reward this episode:  -7.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  109 / 10000\n",
      "Epoch 1/1\n",
      "5565/5565 [==============================] - 3s 551us/step - loss: -0.0100 - acc: 0.9885\n",
      "running reward:  -0.522963422261439  reward this episode:  -7.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  110 / 10000\n",
      "Epoch 1/1\n",
      "5376/5376 [==============================] - 3s 551us/step - loss: -0.0011 - acc: 0.9866\n",
      "running reward:  -0.6477337880388246  reward this episode:  -13.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  111 / 10000\n",
      "Epoch 1/1\n",
      "6185/6185 [==============================] - 4s 569us/step - loss: -0.0092 - acc: 0.9877\n",
      "running reward:  -0.7112564501584364  reward this episode:  -7.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  112 / 10000\n",
      "Epoch 1/1\n",
      "8094/8094 [==============================] - 4s 552us/step - loss: -0.0055 - acc: 0.9883\n",
      "running reward:  -0.714143885656852  reward this episode:  -1.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  113 / 10000\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6207/6207 [==============================] - 3s 556us/step - loss: -9.8167e-04 - acc: 0.9857\n",
      "running reward:  -0.8370024468002836  reward this episode:  -13.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  114 / 10000\n",
      "Epoch 1/1\n",
      "8087/8087 [==============================] - 5s 567us/step - loss: -0.0022 - acc: 0.9857\n",
      "running reward:  -0.8786324223322808  reward this episode:  -5.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  115 / 10000\n",
      "Epoch 1/1\n",
      "8190/8190 [==============================] - 5s 552us/step - loss: -0.0044 - acc: 0.9858\n",
      "running reward:  -0.909846098108958  reward this episode:  -4.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  116 / 10000\n",
      "Epoch 1/1\n",
      "7505/7505 [==============================] - 4s 548us/step - loss: -0.0048 - acc: 0.9892\n",
      "running reward:  -0.9707476371278685  reward this episode:  -7.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  117 / 10000\n",
      "Epoch 1/1\n",
      "5742/5742 [==============================] - 3s 546us/step - loss: -0.0045 - acc: 0.9859\n",
      "running reward:  -1.0810401607565898  reward this episode:  -12.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  118 / 10000\n",
      "Epoch 1/1\n",
      "7404/7404 [==============================] - 4s 552us/step - loss: -0.0050 - acc: 0.9887\n",
      "running reward:  -1.120229759149024  reward this episode:  -5.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  119 / 10000\n",
      "Epoch 1/1\n",
      "6206/6206 [==============================] - 3s 557us/step - loss: -0.0063 - acc: 0.9884\n",
      "running reward:  -1.0190274615575337  reward this episode:  9.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  120 / 10000\n",
      "Epoch 1/1\n",
      "7519/7519 [==============================] - 4s 554us/step - loss: -0.0061 - acc: 0.9848\n",
      "running reward:  -1.0288371869419584  reward this episode:  -2.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  121 / 10000\n",
      "Epoch 1/1\n",
      "5313/5313 [==============================] - 3s 547us/step - loss: -0.0058 - acc: 0.9880\n",
      "running reward:  -1.1085488150725389  reward this episode:  -9.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  122 / 10000\n",
      "Epoch 1/1\n",
      "6273/6273 [==============================] - 3s 557us/step - loss: -0.0042 - acc: 0.9879\n",
      "running reward:  -1.1674633269218135  reward this episode:  -7.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  123 / 10000\n",
      "Epoch 1/1\n",
      "6272/6272 [==============================] - 3s 551us/step - loss: -0.0084 - acc: 0.9820\n",
      "running reward:  -1.2457886936525955  reward this episode:  -9.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  124 / 10000\n",
      "Epoch 1/1\n",
      "6884/6884 [==============================] - 4s 564us/step - loss: -0.0055 - acc: 0.9866\n",
      "running reward:  -1.2733308067160696  reward this episode:  -4.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  125 / 10000\n",
      "Epoch 1/1\n",
      "3626/3626 [==============================] - 2s 555us/step - loss: -0.0054 - acc: 0.9881\n",
      "running reward:  -1.4305974986489087  reward this episode:  -17.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  126 / 10000\n",
      "Epoch 1/1\n",
      "5542/5542 [==============================] - 3s 554us/step - loss: -0.0054 - acc: 0.9901\n",
      "running reward:  -1.3262915236624195  reward this episode:  9.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  127 / 10000\n",
      "Epoch 1/1\n",
      "7347/7347 [==============================] - 4s 569us/step - loss: -0.0070 - acc: 0.9869\n",
      "running reward:  -1.3630286084257954  reward this episode:  -5.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  128 / 10000\n",
      "Epoch 1/1\n",
      "6456/6456 [==============================] - 4s 551us/step - loss: -0.0037 - acc: 0.9890\n",
      "running reward:  -1.4393983223415376  reward this episode:  -9.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  129 / 10000\n",
      "Epoch 1/1\n",
      "5660/5660 [==============================] - 3s 549us/step - loss: -0.0056 - acc: 0.9885\n",
      "running reward:  -1.335004339118122  reward this episode:  9.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  130 / 10000\n",
      "Epoch 1/1\n",
      "7058/7058 [==============================] - 4s 557us/step - loss: -0.0050 - acc: 0.9865\n",
      "running reward:  -1.401654295726941  reward this episode:  -8.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  131 / 10000\n",
      "Epoch 1/1\n",
      "7894/7894 [==============================] - 4s 548us/step - loss: -0.0090 - acc: 0.9871\n",
      "running reward:  -1.3776377527696715  reward this episode:  1.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  132 / 10000\n",
      "Epoch 1/1\n",
      "5465/5465 [==============================] - 3s 543us/step - loss: -0.0051 - acc: 0.9881\n",
      "running reward:  -1.463861375241975  reward this episode:  -10.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  133 / 10000\n",
      "Epoch 1/1\n",
      "6694/6694 [==============================] - 4s 545us/step - loss: -0.0048 - acc: 0.9891\n",
      "running reward:  -1.4992227614895552  reward this episode:  -5.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  134 / 10000\n",
      "Epoch 1/1\n",
      "6956/6956 [==============================] - 4s 551us/step - loss: -0.0031 - acc: 0.9894\n",
      "running reward:  -1.4542305338746597  reward this episode:  3.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  135 / 10000\n",
      "Epoch 1/1\n",
      "7152/7152 [==============================] - 4s 543us/step - loss: -0.0063 - acc: 0.9874\n",
      "running reward:  -1.4696882285359132  reward this episode:  -3.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  136 / 10000\n",
      "Epoch 1/1\n",
      "5891/5891 [==============================] - 3s 544us/step - loss: -0.0101 - acc: 0.9852\n",
      "running reward:  -1.5449913462505542  reward this episode:  -9.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  137 / 10000\n",
      "Epoch 1/1\n",
      "7167/7167 [==============================] - 4s 542us/step - loss: -0.0090 - acc: 0.9819\n",
      "running reward:  -1.5595414327880488  reward this episode:  -3.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  138 / 10000\n",
      "Epoch 1/1\n",
      "5455/5455 [==============================] - 3s 546us/step - loss: -0.0046 - acc: 0.9863\n",
      "running reward:  -1.6739460184601684  reward this episode:  -13.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  139 / 10000\n",
      "Epoch 1/1\n",
      "6574/6574 [==============================] - 4s 552us/step - loss: -0.0038 - acc: 0.9883\n",
      "running reward:  -1.6072065582755668  reward this episode:  5.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  140 / 10000\n",
      "Epoch 1/1\n",
      "5964/5964 [==============================] - 3s 553us/step - loss: -0.0069 - acc: 0.9889\n",
      "running reward:  -1.6711344926928111  reward this episode:  -8.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  141 / 10000\n",
      "Epoch 1/1\n",
      "5731/5731 [==============================] - 3s 548us/step - loss: -0.0039 - acc: 0.9841\n",
      "running reward:  -1.7744231477658832  reward this episode:  -12.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  142 / 10000\n",
      "Epoch 1/1\n",
      "7755/7755 [==============================] - 4s 549us/step - loss: -0.0069 - acc: 0.9876\n",
      "running reward:  -1.7866789162882244  reward this episode:  -3.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  143 / 10000\n",
      "Epoch 1/1\n",
      "5673/5673 [==============================] - 3s 554us/step - loss: -0.0065 - acc: 0.9857\n",
      "running reward:  -1.8688121271253422  reward this episode:  -10.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  144 / 10000\n",
      "Epoch 1/1\n",
      "6869/6869 [==============================] - 4s 547us/step - loss: -0.0057 - acc: 0.9847\n",
      "running reward:  -1.9601240058540887  reward this episode:  -11.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  145 / 10000\n",
      "Epoch 1/1\n",
      "6017/6017 [==============================] - 3s 551us/step - loss: -0.0114 - acc: 0.9840\n",
      "running reward:  -1.8605227657955479  reward this episode:  8.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  146 / 10000\n",
      "Epoch 1/1\n",
      "5702/5702 [==============================] - 3s 567us/step - loss: -0.0023 - acc: 0.9870\n",
      "running reward:  -1.9719175381375922  reward this episode:  -13.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  147 / 10000\n",
      "Epoch 1/1\n",
      "7318/7318 [==============================] - 4s 556us/step - loss: -0.0035 - acc: 0.9889\n",
      "running reward:  -1.9721983627562163  reward this episode:  -2.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  148 / 10000\n",
      "Epoch 1/1\n",
      "6902/6902 [==============================] - 4s 555us/step - loss: -0.0070 - acc: 0.9875\n",
      "running reward:  -2.012476379128654  reward this episode:  -6.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  149 / 10000\n",
      "Epoch 1/1\n",
      "6453/6453 [==============================] - 4s 549us/step - loss: -0.0098 - acc: 0.9864\n",
      "running reward:  -2.0623516153373678  reward this episode:  -7.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  150 / 10000\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5669/5669 [==============================] - 3s 547us/step - loss: -0.0012 - acc: 0.9878\n",
      "running reward:  -2.121728099183994  reward this episode:  -8.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  151 / 10000\n",
      "Epoch 1/1\n",
      "6814/6814 [==============================] - 4s 554us/step - loss: -0.0058 - acc: 0.9862\n",
      "running reward:  -2.230510818192154  reward this episode:  -13.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  152 / 10000\n",
      "Epoch 1/1\n",
      "5892/5892 [==============================] - 3s 548us/step - loss: -0.0075 - acc: 0.9866\n",
      "running reward:  -2.3382057100102323  reward this episode:  -13.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  153 / 10000\n",
      "Epoch 1/1\n",
      "5707/5707 [==============================] - 3s 547us/step - loss: -0.0095 - acc: 0.9886\n",
      "running reward:  -2.26482365291013  reward this episode:  5.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  154 / 10000\n",
      "Epoch 1/1\n",
      "7354/7354 [==============================] - 4s 548us/step - loss: -0.0044 - acc: 0.9871\n",
      "running reward:  -2.222175416381029  reward this episode:  2.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  155 / 10000\n",
      "Epoch 1/1\n",
      "6422/6422 [==============================] - 3s 544us/step - loss: -0.0042 - acc: 0.9880\n",
      "running reward:  -2.339953662217219  reward this episode:  -14.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  156 / 10000\n",
      "Epoch 1/1\n",
      "6833/6833 [==============================] - 4s 541us/step - loss: -0.0108 - acc: 0.9862\n",
      "running reward:  -2.386554125595046  reward this episode:  -7.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  157 / 10000\n",
      "Epoch 1/1\n",
      "6934/6934 [==============================] - 4s 552us/step - loss: -0.0071 - acc: 0.9873\n",
      "running reward:  -2.3926885843390955  reward this episode:  -3.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  158 / 10000\n",
      "Epoch 1/1\n",
      "5756/5756 [==============================] - 3s 547us/step - loss: -0.0104 - acc: 0.9861\n",
      "running reward:  -2.4787616984957044  reward this episode:  -11.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  159 / 10000\n",
      "Epoch 1/1\n",
      "5518/5518 [==============================] - 3s 559us/step - loss: -0.0057 - acc: 0.9884\n",
      "running reward:  -2.563974081510747  reward this episode:  -11.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  160 / 10000\n",
      "Epoch 1/1\n",
      "7114/7114 [==============================] - 4s 554us/step - loss: -0.0046 - acc: 0.9890\n",
      "running reward:  -2.4983343406956395  reward this episode:  4.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  161 / 10000\n",
      "Epoch 1/1\n",
      "7494/7494 [==============================] - 4s 544us/step - loss: -0.0094 - acc: 0.9873\n",
      "running reward:  -2.483350997288683  reward this episode:  -1.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  162 / 10000\n",
      "Epoch 1/1\n",
      "6669/6669 [==============================] - 4s 548us/step - loss: -0.0043 - acc: 0.9861\n",
      "running reward:  -2.508517487315796  reward this episode:  -5.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  163 / 10000\n",
      "Epoch 1/1\n",
      "6533/6533 [==============================] - 4s 545us/step - loss: -0.0085 - acc: 0.9856\n",
      "running reward:  -2.613432312442638  reward this episode:  -13.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  164 / 10000\n",
      "Epoch 1/1\n",
      "6743/6743 [==============================] - 4s 544us/step - loss: -0.0096 - acc: 0.9875\n",
      "running reward:  -2.577297989318212  reward this episode:  1.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  165 / 10000\n",
      "Epoch 1/1\n",
      "3976/3976 [==============================] - 2s 547us/step - loss: -4.3141e-04 - acc: 0.9852\n",
      "running reward:  -2.73152500942503  reward this episode:  -18.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  166 / 10000\n",
      "Epoch 1/1\n",
      "5811/5811 [==============================] - 3s 544us/step - loss: -0.0074 - acc: 0.9873\n",
      "running reward:  -2.8042097593307798  reward this episode:  -10.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  167 / 10000\n",
      "Epoch 1/1\n",
      "6182/6182 [==============================] - 4s 569us/step - loss: -0.0094 - acc: 0.9864\n",
      "running reward:  -2.846167661737472  reward this episode:  -7.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  168 / 10000\n",
      "Epoch 1/1\n",
      "6043/6043 [==============================] - 3s 551us/step - loss: -0.0051 - acc: 0.9876\n",
      "running reward:  -2.907705985120097  reward this episode:  -9.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  169 / 10000\n",
      "Epoch 1/1\n",
      "5939/5939 [==============================] - 3s 540us/step - loss: -0.0041 - acc: 0.9877\n",
      "running reward:  -3.0186289252688963  reward this episode:  -14.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  170 / 10000\n",
      "Epoch 1/1\n",
      "4953/4953 [==============================] - 3s 546us/step - loss: -0.0059 - acc: 0.9873\n",
      "running reward:  -3.138442636016207  reward this episode:  -15.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  171 / 10000\n",
      "Epoch 1/1\n",
      "7408/7408 [==============================] - 4s 558us/step - loss: -0.0093 - acc: 0.9866\n",
      "running reward:  -3.1270582096560453  reward this episode:  -2.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  172 / 10000\n",
      "Epoch 1/1\n",
      "6606/6606 [==============================] - 4s 543us/step - loss: -0.0070 - acc: 0.9861\n",
      "running reward:  -3.1457876275594847  reward this episode:  -5.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  173 / 10000\n",
      "Epoch 1/1\n",
      "6581/6581 [==============================] - 4s 553us/step - loss: -0.0075 - acc: 0.9898\n",
      "running reward:  -3.19432975128389  reward this episode:  -8.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  174 / 10000\n",
      "Epoch 1/1\n",
      "5772/5772 [==============================] - 3s 570us/step - loss: -0.0045 - acc: 0.9860\n",
      "running reward:  -3.282386453771051  reward this episode:  -12.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  175 / 10000\n",
      "Epoch 1/1\n",
      "7679/7679 [==============================] - 4s 548us/step - loss: -0.0037 - acc: 0.9895\n",
      "running reward:  -3.179562589233341  reward this episode:  7.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  176 / 10000\n",
      "Epoch 1/1\n",
      "6166/6166 [==============================] - 3s 558us/step - loss: -0.0059 - acc: 0.9886\n",
      "running reward:  -3.1677669633410077  reward this episode:  -2.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  177 / 10000\n",
      "Epoch 1/1\n",
      "5157/5157 [==============================] - 3s 559us/step - loss: -0.0109 - acc: 0.9888\n",
      "running reward:  -3.0760892937075974  reward this episode:  6.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  178 / 10000\n",
      "Epoch 1/1\n",
      "5213/5213 [==============================] - 3s 542us/step - loss: -0.0055 - acc: 0.9883\n",
      "running reward:  -3.1653284007705214  reward this episode:  -12.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  179 / 10000\n",
      "Epoch 1/1\n",
      "6516/6516 [==============================] - 4s 541us/step - loss: -0.0056 - acc: 0.9894\n",
      "running reward:  -3.1036751167628163  reward this episode:  3.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  180 / 10000\n",
      "Epoch 1/1\n",
      "5609/5609 [==============================] - 3s 542us/step - loss: -0.0082 - acc: 0.9832\n",
      "running reward:  -3.2026383655951878  reward this episode:  -13.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  181 / 10000\n",
      "Epoch 1/1\n",
      "5778/5778 [==============================] - 3s 551us/step - loss: -0.0079 - acc: 0.9893\n",
      "running reward:  -3.2006119819392356  reward this episode:  -3.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  182 / 10000\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 3s 543us/step - loss: -0.0108 - acc: 0.9872\n",
      "running reward:  -3.1386058621198436  reward this episode:  3.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  183 / 10000\n",
      "Epoch 1/1\n",
      "4934/4934 [==============================] - 3s 548us/step - loss: -0.0067 - acc: 0.9899\n",
      "running reward:  -3.0172198034986453  reward this episode:  9.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  184 / 10000\n",
      "Epoch 1/1\n",
      "7313/7313 [==============================] - 4s 539us/step - loss: -0.0033 - acc: 0.9876\n",
      "running reward:  -2.9970476054636586  reward this episode:  -1.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  185 / 10000\n",
      "Epoch 1/1\n",
      "6811/6811 [==============================] - 4s 549us/step - loss: -0.0066 - acc: 0.9893\n",
      "running reward:  -2.877077129409022  reward this episode:  9.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  186 / 10000\n",
      "Epoch 1/1\n",
      "5899/5899 [==============================] - 3s 544us/step - loss: -0.0032 - acc: 0.9853\n",
      "running reward:  -2.9183063581149318  reward this episode:  -7.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  187 / 10000\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7378/7378 [==============================] - 4s 550us/step - loss: -0.0079 - acc: 0.9852\n",
      "running reward:  -3.0191232945337823  reward this episode:  -13.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  188 / 10000\n",
      "Epoch 1/1\n",
      "6857/6857 [==============================] - 4s 545us/step - loss: -0.0097 - acc: 0.9873\n",
      "running reward:  -3.018932061588444  reward this episode:  -3.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  189 / 10000\n",
      "Epoch 1/1\n",
      "5417/5417 [==============================] - 3s 544us/step - loss: 3.2784e-04 - acc: 0.9843\n",
      "running reward:  -3.1187427409725594  reward this episode:  -13.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  190 / 10000\n",
      "Epoch 1/1\n",
      "5634/5634 [==============================] - 3s 556us/step - loss: -0.0024 - acc: 0.9899\n",
      "running reward:  -3.217555313562834  reward this episode:  -13.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  191 / 10000\n",
      "Epoch 1/1\n",
      "6500/6500 [==============================] - 4s 557us/step - loss: -0.0110 - acc: 0.9858\n",
      "running reward:  -3.2553797604272052  reward this episode:  -7.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  192 / 10000\n",
      "Epoch 1/1\n",
      "7115/7115 [==============================] - 4s 550us/step - loss: -0.0077 - acc: 0.9872\n",
      "running reward:  -3.232825962822933  reward this episode:  -1.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  193 / 10000\n",
      "Epoch 1/1\n",
      "7177/7177 [==============================] - 4s 550us/step - loss: -0.0021 - acc: 0.9862\n",
      "running reward:  -3.2904977031947036  reward this episode:  -9.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  194 / 10000\n",
      "Epoch 1/1\n",
      "6578/6578 [==============================] - 4s 548us/step - loss: -0.0064 - acc: 0.9877\n",
      "running reward:  -3.3475927261627563  reward this episode:  -9.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  195 / 10000\n",
      "Epoch 1/1\n",
      "6622/6622 [==============================] - 4s 545us/step - loss: -6.8458e-04 - acc: 0.9890\n",
      "running reward:  -3.3741167989011287  reward this episode:  -6.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  196 / 10000\n",
      "Epoch 1/1\n",
      "8249/8249 [==============================] - 4s 539us/step - loss: -0.0038 - acc: 0.9890\n",
      "running reward:  -3.3003756309121175  reward this episode:  4.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  197 / 10000\n",
      "Epoch 1/1\n",
      "5847/5847 [==============================] - 3s 542us/step - loss: -0.0069 - acc: 0.9911\n",
      "running reward:  -3.1773718746029966  reward this episode:  9.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  198 / 10000\n",
      "Epoch 1/1\n",
      "6629/6629 [==============================] - 4s 553us/step - loss: -0.0077 - acc: 0.9852\n",
      "running reward:  -3.1555981558569663  reward this episode:  -1.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  199 / 10000\n",
      "Epoch 1/1\n",
      "6191/6191 [==============================] - 3s 549us/step - loss: -0.0092 - acc: 0.9866\n",
      "running reward:  -3.244042174298397  reward this episode:  -12.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  200 / 10000\n",
      "Epoch 1/1\n",
      "5643/5643 [==============================] - 3s 549us/step - loss: -0.0063 - acc: 0.9892\n",
      "running reward:  -3.131601752555413  reward this episode:  8.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  201 / 10000\n",
      "Epoch 1/1\n",
      "6524/6524 [==============================] - 4s 566us/step - loss: -0.0040 - acc: 0.9859\n",
      "running reward:  -3.180285735029859  reward this episode:  -8.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  202 / 10000\n",
      "Epoch 1/1\n",
      "6102/6102 [==============================] - 3s 549us/step - loss: -0.0048 - acc: 0.9918\n",
      "running reward:  -3.0984828776795603  reward this episode:  5.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  203 / 10000\n",
      "Epoch 1/1\n",
      "6396/6396 [==============================] - 4s 553us/step - loss: -0.0067 - acc: 0.9881\n",
      "running reward:  -3.1574980489027644  reward this episode:  -9.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  204 / 10000\n",
      "Epoch 1/1\n",
      "5080/5080 [==============================] - 3s 549us/step - loss: -0.0049 - acc: 0.9892\n",
      "running reward:  -3.015923068413737  reward this episode:  11.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  205 / 10000\n",
      "Epoch 1/1\n",
      "6258/6258 [==============================] - 4s 598us/step - loss: -0.0062 - acc: 0.9848\n",
      "running reward:  -3.0957638377295993  reward this episode:  -11.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  206 / 10000\n",
      "Epoch 1/1\n",
      "7281/7281 [==============================] - 4s 548us/step - loss: -0.0018 - acc: 0.9879\n",
      "running reward:  -3.1448061993523035  reward this episode:  -8.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  207 / 10000\n",
      "Epoch 1/1\n",
      "6453/6453 [==============================] - 4s 555us/step - loss: -0.0038 - acc: 0.9912\n",
      "running reward:  -3.1633581373587805  reward this episode:  -5.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  208 / 10000\n",
      "Epoch 1/1\n",
      "5570/5570 [==============================] - 3s 562us/step - loss: -0.0086 - acc: 0.9842\n",
      "running reward:  -3.231724555985193  reward this episode:  -10.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  209 / 10000\n",
      "Epoch 1/1\n",
      "7415/7415 [==============================] - 4s 539us/step - loss: -0.0047 - acc: 0.9875\n",
      "running reward:  -3.219407310425341  reward this episode:  -2.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  210 / 10000\n",
      "Epoch 1/1\n",
      "5874/5874 [==============================] - 3s 542us/step - loss: -0.0110 - acc: 0.9850\n",
      "running reward:  -3.2872132373210876  reward this episode:  -10.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  211 / 10000\n",
      "Epoch 1/1\n",
      "6993/6993 [==============================] - 4s 542us/step - loss: -0.0101 - acc: 0.9866\n",
      "running reward:  -3.244341104947877  reward this episode:  1.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  212 / 10000\n",
      "Epoch 1/1\n",
      "6944/6944 [==============================] - 4s 543us/step - loss: -0.0047 - acc: 0.9888\n",
      "running reward:  -3.1618976938983985  reward this episode:  5.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  213 / 10000\n",
      "Epoch 1/1\n",
      "6402/6402 [==============================] - 4s 561us/step - loss: -0.0072 - acc: 0.9883\n",
      "running reward:  -3.100278716959415  reward this episode:  3.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  214 / 10000\n",
      "Epoch 1/1\n",
      "7771/7771 [==============================] - 4s 543us/step - loss: -0.0055 - acc: 0.9891\n",
      "running reward:  -3.0792759297898202  reward this episode:  -1.0\n",
      "logging into: ./log20181120-142143/\n",
      "episode  215 / 10000\n",
      "Epoch 1/1\n",
      "6527/6527 [==============================] - 4s 544us/step - loss: -0.0090 - acc: 0.9876\n",
      "running reward:  -3.018483170491922  reward this episode:  3.0\n",
      "logging into: ./log20181120-142143/\n"
     ]
    }
   ],
   "source": [
    "# initialize tf session?\n",
    "#sess = tf.InteractiveSession()\n",
    "\n",
    "# cleaning\n",
    "if (os.path.exists('./logs')):\n",
    "    shutil.rmtree('./logs')\n",
    "os.mkdir('./logs')\n",
    "    \n",
    "now = datetime.now()\n",
    "\n",
    "# log directory\n",
    "\n",
    "log_dir = './log' + now.strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "\n",
    "# model initialization\n",
    "model = initialization(nb_hidden_layer_neurons, input_dimensionality, decay_rate, learning_rate)\n",
    "\n",
    "# gym initialization\n",
    "env = gym.make(\"Pong-v0\")\n",
    "observation = env.reset()\n",
    "prev_input = None # what we'll use for the difference between two frames\n",
    "x_train, y_train, rewards = [],[],[] # initialize arrays\n",
    "running_reward = None # exponentially weighted average of the rewards (per episode)\n",
    "reward_sum = 0\n",
    "episode_nb = 0\n",
    "\n",
    "X_train, Y_train, discounted_rewards = np.array([]),np.array([]), np.array([])\n",
    "\n",
    "# load pre-trained model if exist\n",
    "if (resume and os.path.isfile('my_model_weights.h5')):\n",
    "    print(\"loading previous weights\")\n",
    "    model.load_weights('my_model_weights.h5')\n",
    "\n",
    "# add a callback tensorboard object to visualize learning\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='./Graph' + now.strftime(\"%Y%m%d-%H%M%S\") + \"/\", histogram_freq=0,  \n",
    "          write_graph=True, write_images=True)\n",
    "    \n",
    "# main loop\n",
    "while (episode_nb < nb_episodes):\n",
    "    if render: \n",
    "        time.sleep(0.001)\n",
    "        env.render() \n",
    "\n",
    "    # preprocess the observation, set input as difference between images\n",
    "    cur_input = prepro(observation)\n",
    "    x = cur_input - prev_input if prev_input is not None else np.zeros(input_dimensionality)\n",
    "    prev_input = cur_input\n",
    "    \n",
    "    # forward the policy network and sample action according to the proba distribution\n",
    "    proba = model.predict(np.expand_dims(x, axis=1).T) # need to reshape input to do forward in our model\n",
    "    #tflog('proba', proba)\n",
    "    action = UP_ACTION if np.random.uniform() < proba else DOWN_ACTION # only two actions, up and down, encoded by 2 and 3 in the gym env\n",
    "    y = 1 if action == 2 else 0 # we will use our sampled action as a \"label\" for training later\n",
    "\n",
    "    # log the input and label to train later\n",
    "    x_train.append(x)\n",
    "    y_train.append(y)\n",
    "    \n",
    "    # do one step in our environment\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    reward_sum += reward\n",
    "    rewards.append(reward)\n",
    "    \n",
    "    if done: # end of an episode\n",
    "        print(\"episode \", episode_nb + 1, \"/\", nb_episodes)\n",
    "        episode_nb += 1\n",
    "        \n",
    "        # stack all the x_train, y_train and rewards from the current episode\n",
    "        x_train_ep = np.vstack(x_train)\n",
    "        y_train_ep = np.vstack(y_train)\n",
    "        rewards_ep = np.vstack(rewards)\n",
    "        \n",
    "        x_train, y_train, rewards = [],[],[] # reset our variables because the episode ended\n",
    "        \n",
    "        # compute the discounted rewards and normalize it to control variance\n",
    "        discounted_rewards_ep = discount_rewards(rewards_ep)\n",
    "        discounted_rewards_ep -= np.mean(discounted_rewards_ep)\n",
    "        discounted_rewards_ep /= np.std(discounted_rewards_ep)\n",
    "\n",
    "        # (For later) Karpathy computes the gradient directly here, so he can goes and do backprop. If I only do this, I will do the forward twice...\n",
    "        Y_train = np.append(Y_train, y_train_ep)\n",
    "        X_train = x_train_ep if (X_train.size == 0) else np.vstack((X_train, x_train_ep))\n",
    "        discounted_rewards = np.append(discounted_rewards, discounted_rewards_ep)\n",
    "                \n",
    "        if episode_nb % batch_size == 0:\n",
    "            #print(\"discounted_rewards\", discounted_rewards)\n",
    "            #loss = custom_loss(discounted_rewards)\n",
    "            model.fit(x=X_train, y=Y_train, epochs=1, verbose=1, callbacks=[tbCallBack], sample_weight=discounted_rewards)\n",
    "            X_train, Y_train, discounted_rewards = np.array([]),np.array([]), np.array([])\n",
    "        if episode_nb % 50 == 0:    \n",
    "            model.save_weights('my_model_weights' + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '.h5')\n",
    "        \n",
    "        # Log the reward\n",
    "        running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "        print(\"running reward: \", running_reward, \" reward this episode: \", reward_sum)\n",
    "        print(\"logging into:\", log_dir)\n",
    "        tflog('running_reward', running_reward, custom_dir=log_dir)\n",
    "        tflog('reward_sum', reward_sum, custom_dir=log_dir)\n",
    "        \n",
    "        # Reinitialization\n",
    "        observation = env.reset() # resetting our env\n",
    "        reward_sum = 0\n",
    "        prev_input = None # The new episode must not depend of previous frame (from last episode)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputTensor = model.output #Or model.layers[index].output\n",
    "#Then we need to choose the variables that are in respect to the gradient.\n",
    "\n",
    "listOfVariableTensors = model.trainable_weights\n",
    "#or variableTensors = model.trainable_weights[0]\n",
    "#We can now calculate the gradients. It is as easy as the following:\n",
    "\n",
    "gradients = K.gradients(outputTensor, listOfVariableTensors)\n",
    "#To actually run the gradients given an input, we need to use a bit of Tensorflow.\n",
    "\n",
    "trainingExample = X_train\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "evaluated_gradients = sess.run(gradients,feed_dict={model.input:trainingExample})\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluated_gradients[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.absolute(evaluated_gradients[0]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*Y_train, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[42].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
